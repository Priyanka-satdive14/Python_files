{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Web scraping - Wikipedia\n",
      "\n",
      "All link (first 5):\n",
      "Text: Jump to content, URL: #bodyContent\n",
      "Text: Main page, URL: /wiki/Main_Page\n",
      "Text: Contents, URL: /wiki/Wikipedia:Contents\n",
      "Text: Current events, URL: /wiki/Portal:Current_events\n",
      "Text: Random article, URL: /wiki/Special:Random\n",
      "\n",
      "First paragraph text: Web scraping,web harvesting, orweb data extractionisdata scrapingused forextracting datafromwebsites.[1]Web scraping software may directly access theWorld Wide Webusing theHypertext Transfer Protocolor a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using abotorweb crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central localdatabaseorspreadsheet, for laterretrievaloranalysis.\n",
      "\n",
      "No Table of Contents Found.\n"
     ]
    }
   ],
   "source": [
    "#URL of the live webpage to scrape(wikipedia example)\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "\n",
    "#send a Get request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "#Parse the page content with BeautifulSoup using the'lxml' parser\n",
    "soup = BeautifulSoup(response.content,'lxml')\n",
    "\n",
    "#1.Extract the title of the page\n",
    "title = soup.title.string\n",
    "print(f\"Title of the page: {title}\")\n",
    "\n",
    "#2.Extract all  the anchor tags(links)\n",
    "all_links = soup.find_all('a',href = True)\n",
    "print(\"\\nAll link (first 5):\")\n",
    "for link in all_links[:5]:\n",
    "    print(f\"Text: {link.get_text()}, URL: {link['href']}\")\n",
    "    \n",
    "#3.Extract the first paragraph text\n",
    "first_paragraph = soup.find('p').get_text(strip=True)\n",
    "print(f\"\\nFirst paragraph text: {first_paragraph}\")\n",
    "\n",
    "#4.Extract the sidebar table of contents(if present)\n",
    "toc = soup.find('div',class_='toc')\n",
    "if toc:\n",
    "    toc_text = toc.get_text(separator=\"\\n\",strip=True)\n",
    "    print(f\"\\nTable of Contents:\\n{toc_text}\")\n",
    "else:\n",
    "    print(\"\\nNo Table of Contents Found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secret_key = config.get_key('unsplash_secret_key')\n",
    "def download_image(url):\n",
    "    dir = './images'\n",
    "    photo_name = url.split('?')[0] # before question mark\n",
    "    photo_name = photo_name.split('/')[-1]# last one\n",
    "    photo_name = os.path.join(dir,f'{photo_name}.png')\n",
    "    response = requests.get(url)\n",
    "    with open(photo_name,'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images for the query 'wild':\n",
      "\n",
      "Image 1: https://images.unsplash.com/photo-1474511320723-9a56873867b5?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHwxfHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 2: https://images.unsplash.com/photo-1511208687438-2c5a5abb810c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHwyfHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 3: https://images.unsplash.com/photo-1534759846116-5799c33ce22a?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHwzfHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 4: https://images.unsplash.com/photo-1516934024742-b461fba47600?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw0fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 5: https://images.unsplash.com/photo-1425141750113-187b6a13e28c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw1fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 6: https://images.unsplash.com/photo-1470165511815-34e78ff7a111?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw2fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 7: https://images.unsplash.com/photo-1544985361-b420d7a77043?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw3fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 8: https://images.unsplash.com/photo-1511216113906-8f57bb83e776?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw4fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 9: https://images.unsplash.com/photo-1562569633-622303bafef5?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHw5fHx3aWxkfGVufDB8fHx8MTcyODkxMzkxNnww&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 10: https://images.unsplash.com/photo-1503066211613-c17ebc9daef0?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MjB8MHwxfHNlYXJjaHwxMHx8d2lsZHxlbnwwfHx8fDE3Mjg5MTM5MTZ8MA&ixlib=rb-4.0.3&q=80&w=1080\n"
     ]
    }
   ],
   "source": [
    "#search query for the type of image you want\n",
    "query ='wild'\n",
    "\n",
    "#make a GET request to the unplash API\n",
    "client_id='sEwH8f8qs5Ze7rvZVqO_Q8LyflOokHyIIxgBQlxebx8'\n",
    "url = f'https://api.unsplash.com/search/photos?query={query}&client_id={client_id}'\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "#extract image URLs from the response\n",
    "if 'results' in data:\n",
    "    print(f\"Found {len(data['results'])} images for the query '{query}':\\n\")\n",
    "    for i, photo in enumerate(data['results']):\n",
    "        image_url = photo['urls']['regular']\n",
    "        print(f\"Image {i+1}: {image_url}\")\n",
    "        download_image(image_url)\n",
    "else:\n",
    "    print(\"no result found\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
